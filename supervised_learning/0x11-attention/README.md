<p align="center">
  <a href=#>
    <img src="https://intranet.hbtn.io/assets/holberton-logo-full-black-157ccfa3d2134776c1e3f78c0fe682968e8848b64fcacc6187976044f75f35a8.png" alt="Holberton School logo">
  </a>
</p>

# 0x11. Attention

## Table of Contents
* [About](#about)
* [Files](#files)
* [Author](#author)

## About
In this project, I learned about **transformer models** and the **attention mechanism.**

## Files
* `0-rnn_encoder.py`
* `1-self_attention.py`
* `2-rnn_decoder.py`
* `4-positional_encoding.py`
* `5-sdp_attention.py`
* `6-multihead_attention.py`
* `7-main`
* `7-transformer_encoder_block.py`
* `8-transformer_decoder_block.py`
* `9-transformer_encoder.py`
* `10-transformer_decoder.py`
* `11-transformer.py`
* `README.md`

## Author
Justin Masayda [@keysmusician](https://github.com/keysmusician)
<pre align="center">
        _   _       _   _   _       _   _       _   _   _     
    ___//|_//|_____//|_//|_//|_____//|_//|_____//|_//|_//|___ 
   /  /// ///  /  /// /// ///  /  /// ///  /  /// /// ///  / |
  /  ||/ ||/  /  ||/ ||/ ||/  /  ||/ ||/  /  ||/ ||/ ||/  / / 
 /___/___/___/___/___/___/___/___/___/___/___/___/___/___/ /  
 |___|___|___|___|___|___|___|___|___|___|___|___|___|___|/   
 
</pre>
<p><span style="font-family: 'Lucida Console'; line-height: 14px; font-size: 14px; display: inline-block;">&nbsp;</span></p>
